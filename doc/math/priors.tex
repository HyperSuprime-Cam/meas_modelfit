\documentclass{amsart}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage{bm}
\usepackage[colorlinks=true, urlcolor=blue, citecolor=blue, linkcolor=blue, book
marks=true, ]{hyperref}
\usepackage[top=3cm,right=3cm,left=3cm]{geometry}

\DeclareMathOperator{\erf}{erf}
\newcommand{\D}[2]{\ensuremath{\frac{d{#1}}{d{#2}}}}
\newcommand{\Dp}[2]{\ensuremath{\frac{\partial{#1}}{\partial{#2}}}}

\begin{document}

\section{Definitions}

\begin{itemize}
\item $x$: weighted pixel vector
\item $\Sigma$: pixel covariance matrix (assumed to be diagonal)
\item $\phi$: nonlinear parameters
\item $\mu$: linear parameters (coefficients)
\item $N$: number of coefficients
\end{itemize}

The coefficient prior is defined as

\begin{equation}
  P(\mu|\phi) = g(\mu)
\end{equation}

with the additional constraint $g(\mu) = 0$ for all $\mu$ such that
$\mu^T B^{-1} \mu > 1$, where $B$ is a diagonal matrix.

\section{Bayesian Probabilities}

\begin{eqnarray}
  P(\mu,\phi|x) &=& P(x|\mu,\phi)
  \frac{P(\mu,\phi)}{P(x)} \\
  P(\phi|x)P(\mu|x,\phi) &=&
  P(x|\mu,\phi)\frac{P(\mu|\phi) P(\phi)}{P(x)}
  \\
  P(\phi|x) &=& \frac{P(\phi)}{P(x)}
  \int d\mu P(\mu|\phi) P(x|\mu,\phi) \\
  &=& \frac{P(\phi)}{P(x)} P(x|\phi)
\end{eqnarray}

\section{Coefficient Marginalization}

\begin{eqnarray}
  -\ln P(x|\mu,\phi) &=& \frac{1}{2}(A\mu-x)^T(A\mu-x)
  +\frac{1}{2}\ln\left|2\pi\Sigma\right| \\
  &=& \frac{1}{2}(A\hat{\mu}-x)^T(A\hat{\mu}-x)
  +\frac{1}{2}\ln\left|2\pi\Sigma\right| 
  +\frac{1}{2}(\mu - \hat{\mu})^T\!A^T\!A\,(\mu-\hat{\mu})
\end{eqnarray}

with $\hat{\mu}$ the minimum-norm maximum least-squares solution:

\begin{equation}
  A^T A \hat{\mu} = A^T x
\end{equation}

Because $A$ may not have full rank, we will assume the $\hat{\mu}$
will be accomplished by a singular value decomposition of $A$ (or
equivalently, a spectral decomposition of $A^T\!A$):

\begin{eqnarray}
  A &=& U S Q^T \\
  &=& 
  \left[\begin{array}{ c c }
      U _1 & U_2
      \end{array}\right]
  \left[\begin{array}{ c c }
      S_1 & 0 \\
      0 & 0
    \end{array}\right]
  \left[\begin{array}{ c }
      Q_1^T \\
      Q_2^T
    \end{array}\right]
  \\
  A^T\!A &=& Q S^2 Q^T \\
  &=& 
  \left[\begin{array}{ c c }
      Q_1 & Q_2
    \end{array}\right]
  \left[\begin{array}{ c c }
      S_1^2 & 0 \\
      0 & 0
    \end{array}\right]
  \left[\begin{array}{ c }
      Q_1^T \\
      Q_2^T
    \end{array}\right]  
\end{eqnarray}

\begin{eqnarray}
  P(x|\phi) &=& \int P(x|\mu,\phi)\,P(\mu|\phi)\,d\mu \\
  &=& 
  \frac{e^{-\frac{1}{2}(A\hat{\mu}-x)^T(A\hat{\mu}-x)}}
       {\sqrt{\left|2\pi\Sigma\right|}}
  \int_{\lVert\mu\rVert \le r} \!\!\!\!\!\!\!\!\!\!\!\!
  e^{-\frac{1}{2}(\mu-\hat{\mu})^T\!A^T\!A\,(\mu-\hat{\mu})}\,g(\mu)\,d\mu
\end{eqnarray}

\subsection{Case 1: $A$ has full rank}

\begin{equation}
  \hat{\mu} = Q S^{-1} U^T x
\end{equation}

The likelihood is a well-defined Gaussian.  We draw $M_{\mathrm{total}}$
samples $\mu_i$ from a normal distribution with mean $\hat{\mu}$ and
covariance $Q S^{2} Q^T$ until we have $M_{\mathrm{valid}}$ samples that meet
the constraint $\lVert \mu \rVert \le r$:

\begin{eqnarray}
  P(x|\phi) &\approx& 
  \frac{e^{-\frac{1}{2}(A\hat{\mu}-x)^T(A\hat{\mu}-x)}}
       {\sqrt{\left|2\pi\Sigma\right|}}
  \frac{\sqrt{\left|2\pi S^{-1}\right|}}{M_{\mathrm{total}}}
  \sum_{i=1}^{M_{\mathrm{valid}}} g(\mu_i)
\end{eqnarray}

\subsection{Case 2: $A$ is rank-deficient}

\begin{equation}
  \hat{\mu} = Q_1 S_1^{-1} U_1^T x
\end{equation}

We change integration variables from $\mu$ to 

\begin{equation}
  \nu = 
  \left[\begin{array}{ c }
      \nu_1 \\
      \nu_2
    \end{array}\right]
  =
  \left[\begin{array}{ c }
      Q_1^T\mu \\
      Q_2^T\mu
    \end{array}\right]
  =
  Q^T \mu
\end{equation}

with inverse

\begin{equation}
  \mu = Q\nu = Q_1\nu_1 + Q_2\nu_2
\end{equation}

\begin{eqnarray}
  P(x|\phi) &=&
  \frac{e^{-\frac{1}{2}(A\hat{\mu}-x)^T(A\hat{\mu}-x)}}
       {\sqrt{\left|2\pi\Sigma\right|}}
  \int_{\lVert\nu\rVert \le r} \!\!\!\!\!\!\!\!\!\!\!\!
  e^{-\frac{1}{2}(\nu-\hat{\nu_1})^T\!S_1\,(\nu_1-\hat{\nu_1})}\,g(Q\nu)\,d\nu
\end{eqnarray}

We then draw from a hybrid normal/uniform distribution $f(\nu)$ with
the following procedure:
\begin{enumerate}
\item Draw $M_{\mathrm{total}}$ samples $\nu_{1,i}$ from a normal
  distribution with mean $\hat{\nu}_1$ and diagonal covariance
  $S_1^{-1}$.  Keep only the $M_{\mathrm{valid}}$ samples that satisfy
  $\lVert \nu_{1,i} \rVert \le r$.
\item For each $\nu_{1,i}$ draw $\nu_{2,i}$ from a uniform distribution on a 
  hypersphere with radius $\sqrt{r^2 - \lVert \nu_1 \rVert^2}$
\end{enumerate}

This distribution has the density

\begin{equation}
  f(\nu) = \frac{M_{\mathrm{valid}}}{M_{\mathrm{total}}}
  \frac{e^{-\frac{1}{2}(\nu_1-\hat{\nu}_1)^T S_1^{-1} (\nu_1-\hat{\nu}_1)}}
       {\sqrt{\lvert 2\pi S_1^{-1} \rvert}
         \;\;V\!\left(\sqrt{r^2 - \lVert \nu_1 \rVert^2}, N_2\right)}
\end{equation}

where $V(r,d)$ is the volume of a hypersphere of dimension $d$ and
radii $r$:

\begin{equation}
  \frac{\pi^{d/2}r^d}{\Gamma(\frac{d}{2} + 1)}
\end{equation}

The integral estimator is then

\begin{eqnarray}
  P(x|\phi) &\approx& 
  \frac{e^{-\frac{1}{2}(A\hat{\mu}-x)^T(A\hat{\mu}-x)}}
       {\sqrt{\left|2\pi\Sigma\right|}}
  \frac{\sqrt{\left|2\pi S_1^{-1}\right|}}{M_{\mathrm{total}}}
  \sum_{i=1}^{M_{\mathrm{valid}}} g(Q\nu_i)\;
  V\!\left(\sqrt{r^2 - \lVert \nu_{1,i} \rVert^2}, N_2\right)
\end{eqnarray}

\subsection{Case 3: Low Overlap}

If $r \lvert S_1 \rvert^{1/N_1}$ or
$\frac{M_{\mathrm{valid}}}{M_{\mathrm{total}}}$ is small 
in either of the above cases (where $S_1=S$ and $N_1=N$ in case 1),
drawing samples from a uniform distribution on a hypersphere with
radius $r$ and dimension $N$ may provide an estimator with reduced variance:

\begin{eqnarray}
  P(x|\phi) &\approx& 
  \frac{e^{-\frac{1}{2}(A\hat{\mu}-x)^T(A\hat{\mu}-x)}}
       {\sqrt{\left|2\pi\Sigma\right|}}
  \frac{V(r,N)}{M}
  \sum_{i=1}^{M} e^{-\frac{1}{2}(\mu_i-\hat{\mu})^T\!A^T\!A\,(\mu_i-\hat{\mu})}\,g(\mu_i)
\end{eqnarray}

\end{document}
