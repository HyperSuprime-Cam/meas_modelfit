// -*- lsst-c++ -*-

namespace lsst { namespace meas { namespace multifit {

/**
 *  @page multifitTruncatedGaussianMath Truncated Gaussian Math
 *
 *  The implementation of the TruncatedGaussian class is a bit opaque due to the
 *  complex mathematics involved.  This page provides formulae and explanations
 *  to help explain how it works.
 *
 *  Except where otherwise noted, we'll assume 2 dimensions here, as 1-d is trivial and higher
 *  dimensions are not supported.
 *
 *  We will denote by @f$\Omega@f$ the region in @f$\mathbb{R}^2@f$ such that all dimensions are
 *  positive, and use @f$\Gamma@f$ for its boundary, which consists of the coordinate axes and
 *  an arbitrary path joining them at infinity (which we will regularly ignore since all the
 *  functions were are concerned with are zero at infinity).
 *
 *  @section multifitTruncatedGaussianStandardIntegral Standard-Parameter Integrals
 *
 *  In the standard parameters form, we are modeling the function
 *  @f[
 *    f(\alpha) = \frac{1_{\Omega}(\alpha)}{k\left|2\pi\Sigma\right|^{1/2}}
 *      e^{-\frac{1}{2}(\alpha-\mu)^T \Sigma^{-1} (\alpha-\mu)}
 *  @f]
 *  Where @f$1_{\Omega}(\alpha)@f$ is the indicator function
 *  @f[
 *    1_{\Omega}(\alpha) \equiv \begin{cases}
 *       1 & \text{if $\alpha \in \Omega$}\\
 *       0 & \text{if $\alpha$ otherwise}
 *    \end{cases}
 *  @f]
 *  and @f$k@f$ is the normalization constant that ensures that @f$f(\alpha)@f$ integrates to unity:
 *  @f[
 *    k \equiv \int_\!\!d\Omega\;
 *       \frac{1}{\left|2\pi\Sigma\right|^{1/2}}\;
 *       e^{-\frac{1}{2}(\alpha-\mu)^T \Sigma^{-1} (\alpha-\mu)}
 *  @f]
 *  we assume that the matrix @f$\Sigma@f$ is full-rank (and throw
 *  an exception if it is not), because @f$k@f$ is infinite if @f$\sigma@f$ is singular.
 *
 *  In this case, there is no analytic integral, and we use a translation/reimplementation
 *  of the "bvnu" MatLab routine by Alan Genz to do the numerical integration (see detail::bvnu).
 *
 *  However, bvnu takes its arguments in a different form, computing the integral
 *  @f[
 *    \text{bvnu}(h, k, \rho) = \frac{1}{2\pi\sqrt{1-\rho^2}}\int_h^{\infty}dx\int_k^{\infty}dy
 *        \;e^{-(x^2 - 2\rho x y + y^2)/(2(1-\rho^2))}
 *  @f]
 *  Computing the correlation coefficient @f$\rho@f$ from the covariance matrix is straightforward:
 *  @f[
 *    \rho \equiv \frac{\Sigma_{1,2}}{\sqrt{\Sigma_{1,1} \Sigma_{2,2}}}
 *  @f]
 *  and transforming to the new form is simply a matter of making the substitution
 *  @f[
 *    \eta_1 = \frac{\alpha_1 - \mu_1}{\sqrt{\Sigma_{1,1}}}
 *  @f]
 *  @f[
 *    \eta_2 = \frac{\alpha_2 - \mu_2}{\sqrt{\Sigma_{2,2}}}
 *  @f]
 *  Since both integrals are normalized in the untruncated case, we have
 *  @f[
 *    \frac{1}{\left|2\pi\Sigma\right|}\int_0^{\infty}d\alpha_1\,\int_0^{\infty}d\alpha_2\,
 *    e^{-\frac{1}{2}(\alpha-\mu)^T\,\Sigma^{-1}\,(\alpha-\mu)}
 *    = \text{bvnu}\left(-\frac{\mu_1}{\sqrt{\Sigma_{1,1}}}, -\frac{\mu_2}{\sqrt{\Sigma_{2,2}}},
 *                       \frac{\Sigma_{1,2}}{\sqrt{\Sigma_{1,1} \Sigma_{2,2}}}\right)
 *  @f]
 *
 *  @section multifitTruncatedGaussianSeriesIntegral Series-Parameter Integrals
 *
 *  In the series form, we are modeling the function
 *  @f[
 *    f(\alpha) = 1_{\Omega}(\alpha) \; e^{-q(0) -g^T \alpha -\frac{1}{2}\alpha^T H \alpha}
 *              = 1_{\Omega}(\alpha) \; e^{-q(\alpha)}
 *  @f]
 *
 *  There are two distinct cases, depending on whether @f$H@f$ is singular.  In both cases,
 *  we begin by compute an eigensystem decomposition of @f$H@f$:
 *  @f[
 *    H = V\,S\,V^T
 *  @f]
 *  where @f$S@f$ is diagonal and @f$V@f$ is orthogonal.  We use this to determine whether @f$H@f$
 *  is singular, and if so, to compute its inverse and determinant.  In the nonsingular case, we
 *  can then use the same bvnu routine used for the standard parameters form, noting that
 *  @f[
 *    \mu = -H^{-1} g
 *  @f]
 *  @f[
 *    \Sigma_{11} = \frac{H_{22}}{|H|}
 *  @f]
 *  @f[
 *    \Sigma_{22} = \frac{H_{11}}{|H|}
 *  @f]
 *  @f[
 *    \rho = -\frac{H_{12}}{\sqrt{H_{11}H_{22}}}
 *  @f]
 *
 *  The singular case in the series form can arise when a linear combination of the parameters is
 *  completely unconstrained, but the total amplitude is well constrained (and hence the integral
 *  of the TruncatedGaussian is bounded due to the truncation).  This happens extremely
 *  often, when a galaxy model has two components and the radii of both approach zero.
 *
 *  In the singular case (which bvnu does not handle), we assume the eigensystem is structured as
 *  @f[
 *    H = V\,S\,V^T
 *      = \left[\begin{array}{c c}
 *          V_{1,1} & V_{1,2} \\
 *          V_{2,1} & V_{2,2} \\
 *        \end{array}\right]
 *        \left[\begin{array}{c c}
 *          0 & 0 \\
 *          0 & S_2
 *        \end{array}\right]
 *       \left[\begin{array}{c c}
 *          V_{1,1} & V_{2,1} \                 \
 *          V_{1,2} & V_{2,2}
 *        \end{array}\right]
 *      = \left[\begin{array}{c c}
 *          V_{:,1}
 *          V_{:,2}
 *        \end{array}\right]
 *        \left[\begin{array}{c c}
 *          0 & 0 \\
 *          0 & S_2
 *        \end{array}\right]
 *      = \left[\begin{array}{c}
 *          V_{:,1}^T
 *          V_{:,2}^T
 *        \end{array}\right]
 *  @f]
 *  and we change variables to isolate the zero eigenvalue:
 *  @f[
 *    \alpha = V\beta = V_{:,1} \beta_1 + V_{:,2} \beta_2
 *  @f]
 *  @f[
 *    \beta = V^T\alpha
 *      = \left[\begin{array}{c}
 *           V_{:,1}^T\alpha \\
 *           V_{:,2}^T\alpha
 *        \end{array}\right]
 *      = \left[\begin{array}{c}
 *           \beta_1 \\
 *           \beta_2
 *        \end{array}\right]
 *  @f]
 *  We will assume that @f$V_{1,2}@f$ and @f$V_{2,2}@f$ are both positive (both negative would be an
 *  equally valid assumption, just a different convention, but mixed sign would indicate a
 *  divergent integral), and that @f$V_{:,1}^T g=0@f$ (this is equivalent to stating that the
 *  singularity arises because the least-squares problem represented by the series expansion
 *  has many degenerate solutions).
 *
 *  The region of integration defined by @f$\Omega@f$, in the rotated @f$\beta@f$ frame,
 *  lies between the lines that represent the rotated @f$\alpha_1@f$- and @f$\alpha_2@f$-axes,
 *  which are
 *  @f[
 *    \beta_1 = -\frac{V_{1,2}}{V_{1,1}}\beta_2
 *  @f]
 *  and
 *  @f[
 *    \beta_1 = -\frac{V_{2,2}}{V_{2,1}}\beta_2
 *  @f]
 *  with @f$\beta_2 \ge 0@f$.  We can thus write the integral in @f$\beta@f$ as
 *  @f[
 *    \int_{0}^{\infty} d\beta_2 e^{-q(0) - g^T V_{:,2} \beta_2 -\frac{1}{2}S_2\beta_2^2}
 *    \int_{-\frac{V_{1,2}}{V_{1,1}}\beta_2}^{-\frac{V_{2,2}}{V_{2,1}}\beta_2} d\beta_1
 *    =\left(\frac{V_{1,2}}{V_{1,1}}-\frac{V_{2,2}}{V_{2,1}}\right)
 *    \int_{0}^{\infty} d\beta_2\;\beta_2\;e^{-q(0) - g^T V_{:,2} \beta_2 -\frac{1}{2}S_2\beta_2^2}
 *  @f]
 *
 *  This can now be integrated using integration by parts and standard 1-d Gaussian integrals
 *  (I used Mathematica) to yield
 *  @f[
 *    \left(\frac{V_{1,2}}{V_{1,1}}-\frac{V_{2,2}}{V_{2,1}}\right)\frac{e^{-q(0)}}{S_2}\left[
 *      1 - z\sqrt{\pi}\;e^{z^2}\;\text{erfc}\left(z\right)
 *    \right]
 *  @f]
 *  where
 *  @f[
 *    z \equiv \frac{V_{:,2}^T\,g}{\sqrt{2S_2}}
 *  @f]
 *
 *  @section multifitTruncatedGaussianSampling Sampling
 *
 *  TruncatedGaussian supports two strategies for sampling, set by the
 *  TruncatedGaussian::SampleStrategy enum.  The "direct-with-rejection" strategy is quite simple,
 *  and needs no further development here (see the enum docs for more information).  The
 *  "align-and-weight" strategy is a bit more complex.
 *
 *  With this strategy, instead of sampling from the untruncated Gaussian, we use importance sampling:
 *  we draw samples from a similar "importance" distribution, then weigh these by the ratio of the
 *  true distribution to the importance distribution at each point.  For the importance distribution,
 *  we'll use an "axis-aligned" Gaussian (i.e. one with a diagonal covariance matrix); this will allow
 *  us to draw from each dimension independently.  Instead of doing this by drawing directly from the
 *  1-d Gaussian and rejecting, however, we'll draw directly from the truncated 1-d Gaussian using its
 *  inverse CDF.
 *
 *  In detail, then, we start with a function @f$f(\alpha)@f$, for which we know the total integral
 *  @f[
 *    A_f = \int\!\!d\alpha\;f(\alpha)
 *  @f]
 *  (note that this includes the truncation), as well as the Hessian matrix @f$H@f$ and mean @f$\mu@f$.
 *  This mixing of the series and standard parametrizations is useful because we can always compute
 *  @f$H@f$ from @f$\Sigma@f$ because we assume the latter is always nonsingular, but we do not assume
 *  that @f$H@f$ is nonsingular.  Similarly, we can compute @f$\mu=-H^+g@f$ from the series form.
 *  Because one logically draws from normalized probability distributions, not arbitrarily scaled
 *  functions, we are thus logically drawing from @f$f(\alpha)/A_f@f$, not simply @f$f(\alpha)@f$.
 *
 *  To construct the importance distribution, we'd like to use the diagonal of @f$H^{-1}=\Sigma@f$,
 *  but because we cannot guarantee that @f$H@f$ is nonsingular, we will instead use the inverse of
 *  the diagonal of @f$H@f$.  This represents the width in each dimension at fixed values in the
 *  other dimensions (note that the diagonal of @f$\Sigma@f$ would represent the marginal values
 *  if the Gaussian were not truncated), so it's not quite wide enough to make a good importance
 *  distribution.  To address this, we'll simply multiply by 2, so the importance distribution
 *  will have the diagonal covariance matrix @f$D@f$ defined by:
 *  @f[
 *    D_i = 2\sqrt{H_{i,i}}
 *  @f]
 *  We could do much more clever things by computing the actual confidence limits of the truncated
 *  Gaussian, but the current approach seems to work well enough so it doesn't seem to be worth the
 *  extra work.
 *
 *  Our importance function is thus
 *  @f[
 *    p(\alpha) = 1_{\Omega}(\alpha)\;\frac{1}{2\pi|D|^{1/2}}\;
 *                   e^{-\frac{1}{2}\left(\alpha-\mu\right)^T\,D\,\left(\alpha-\mu\right)}
 *        = \prod_i p(\alpha_i)
 *  @f]
 *  @f[
 *    p(\alpha_i) = \frac{1_{\alpha_i\ge 0}}{\sqrt{2\pi D_i}}\;e^{-\frac{1}{2D_i}(\alpha_i-\mu_i)^2}
 *  @f]
 *  with normalization factor
 *  @f[
 *    A_p = \int\!\! d\alpha\;p(\alpha) = \prod_i A_{p_i}
 *  @f]
 *  @f[
 *    A_{p_i} = \int_0^{\infty}\!\!d\alpha_i\;p(\alpha_i)
 *         = \frac{1}{2}\text{erfc}\left(\frac{-\mu_i}{\sqrt{2D_i}}\right)
 *         = \Phi\left(\frac{\mu_i}{\sqrt{D_i}}\right)
 *         = 1-\Phi\left(\frac{-\mu_i}{\sqrt{D_i}}\right)
 *  @f]
 *
 *  To draw from @f$p(\alpha)@f$, we draw from each @f$r(\alpha_i)@f$ in turn, which can be accomplished
 *  via
 *  @f[
 *    \alpha_i = \Phi^{-1}\left(\Phi\left(\frac{-\mu_i}{D_i}\right)
 *                              + u\left[1 - \Phi\left(\frac{-\mu_i}{D_i}\right) \right]\right)
 *                              \sqrt{D_i} + \mu_i
 *       = \Phi^{-1}\left(1 - A_{p_i} + u A_{p_i}\right)\sqrt{D_i} + \mu_i
 *       = -\sqrt{2}\,\text{erfc}^{-1}\left(2\left[1-u A_{p_i}\right]\right)\sqrt{D_i} + \mu_i
 *  @f]
 *  where @f$u@f$ is a uniform random variate on @f$(0,1)@f$, and we have used the fact that
 *  @f$1-u@f$ and @f$-u@f$ are both random variates on @f$(-1,0)@f$.
 *
 *  After drawing these from the importance distribution, we compute the weight for each point as
 *  @f[
 *   w = \frac{A_p\;f(\alpha)}{A_f\;f(\alpha)}
 *  @f]
 *
 *  @section multifitTruncatedGaussianMaximization Maximization
 *
 *  To derive the formulae for higher-order moments, we will use the same hybrid functional form
 *  as in the sampling section:
 *  @f[
 *     f(\alpha) = e^{-\frac{1}{2}(\alpha-\mu)^T H (\alpha-\mu)}
 *  @f]
 *  Our goal is to solve the quadratic programming problem
 *  @f[
 *   \max_\alpha f(\alpha) = \min_\alpha (\alpha-\mu)^T H (\alpha-\mu)
 *  @f]
 *  subject to
 *  @f[
 *   \alpha \ge 0
 *  @f]
 *  In general quadratic programming, the difficulty is in knowing which of the inequality constraints
 *  will be active at the solution, and "active set" methods work by adding and removing constraints
 *  as they explore the problem.  In our case, the constraints are sufficiently simple (and the objective
 *  function is known to be convex), so we can simply identify the active set at the solution as the
 *  dimensions for which the untruncated maximum point (i.e. @f$\mu@f$) has a negative value.
 *
 *  We begin simply by iterating over the vector @f$\mu@f$, identifying an elements that are less than
 *  zero.  For each such element @f$\mu_i@f$, we add a unit vector row to a permutation matrix @f$A@f$
 *  such that @f$A^T\mu@f$ selects the components of @f$\mu@f$ that are less than zero.  We can now
 *  solve the equality-constrained quadratic problem
 *  @f[
 *   \min_\alpha (\alpha-\mu)^T H (\alpha-\mu)
 *  @f]
 *  subject to
 *  @f[
 *   A^T\alpha = 0
 *  @f]
 *
 *  We start by computing the QR decomposition of @f$A@f$:
 *  @f[
 *   A = Q R = \left[\begin{array}{c c}
 *                 Q_1 & Q_2
 *             \end{array}\right]
 *             \left[\begin{array}{c}
 *                 R_1 \\
 *                 0
 *             \end{array}\right]
 *  @f]
 *  We then perform the change of variables
 *  @f[
 *   \tau = Q^T \alpha = \left[\begin{array}{c}
 *                           Q_1^T \alpha \\
 *                           Q_2^T \alpha
 *                         \end{array}\right]
 *                       = \left[\begin{array}{c}
 *                           \tau_1 \\
 *                           \tau_2
 *                         \end{array}\right]
 *  @f]
 *  with inverse
 *  @f[
 *    \alpha = Q\tau = Q_1\tau_1 + Q_2\tau_2
 *  @f]
 *  The constraint system reduces to @f$\tau_1=0@f$, which we can then plug into the objective function:
 *  @f[
 *    \min_{\tau_1} (Q_1\tau_1 - \mu)^T H (Q_1\tau_1 - \mu)
 *  @f]
 *  Differentiating by @f$\tau_1@f$ and setting the result to zero, we have:
 *  @f[
 *   2 H (Q_1\tau_1 - \mu) = 0
 *  @f]
 *  or simply (because @f$Q_1^T Q_1 = I@f$)
 *  @f[
 *   \tau_1 = Q_1^T\mu
 *  @f]
 *  The corresponding value of @f$\alpha@f$ is then
 *  @f[
 *   \hat{\alpha} = Q_1 \tau_1 = Q_1 Q_1^T \mu
 *  @f]
 *
 *  @section multifitTruncatedGaussianMoments Moments
 *
 *  To derive the formulae for higher-order moments, we will use the same hybrid functional form
 *  as in the sampling section:
 *  @f[
 *     f(\alpha) = e^{-\frac{1}{2}(\alpha-\mu)^T H (\alpha-\mu)}
 *  @f]
 *  As we are interested in normalized moments, we will not include an additional arbitrary scaling, and
 *  we will include the truncation in the region of integration; the moments we are interested
 *  in are:
 *  @f[
 *     m_0 = \int\!\!d\Omega\;f(\alpha)
 *  @f]
 *  @f[
 *     m_1 = \frac{1}{m_0}\int\!\!d\Omega\;\alpha\;f(\alpha)
 *  @f]
 *  @f[
 *     M_2 = \frac{1}{m_0}\int\!\!d\Omega\;(\alpha - m_1)(\alpha - m_1)^T\;f(\alpha)
 *  @f]
 *
 *  Clearly, @f$m_0@f$ is trivially related to the integrals discussed above, so we will start on
 *  @f$m_1@f$ by noting that
 *  @f[
 *     m_0 H (m_1-\mu) = \int\!\!d\Omega\;(H\alpha-\mu)\;f(\alpha) = -\int\!\!d\Omega\;\nabla f(\alpha)
 *     \equiv \hat{m}_1
 *  @f]
 *  This surface integral is equivalent to the lower-dimensional integral
 *  @f[
 *     \hat{m}_1 = \int\!\!d\Gamma\;\hat{n}\;f(\alpha)
 *  @f]
 *  where @f$\hat{n}@f$ is a unit vector normal to @f$\Gamma@f$, pointing outwards from the center of
 *  @f$\Omega@f$.  In 2-d, this line integral can be split into three components:
 *   - an integral along @f$\alpha_1=0@f$ from @f$\alpha_2=0@f$ to @f$\alpha_2=\infty@f$, with
 *     @f$\hat{n}_1=-1@f$ and @f$\hat{n}_2=0@f$.
 *   - an integral along @f$\alpha_2=0@f$ from @f$\alpha_1=0@f$ to @f$\alpha_1=\infty@f$, with
 *     @f$\hat{n}_1=0@f$ and @f$\hat{n}_2=-1@f$.
 *   - an integral along a connecting path at infinity we can ignore because the integrand is zero there.
 *  Thus, for component @f$i@f$ (and @f$j \ne i@f$), we have
 *  @f$[
 *    \left[\hat{m}_1\right]_i = \int\!\!d\alpha_j\;f(\alpha)|_{\alpha_i=0}
 *  @f$]
 */
}}}
